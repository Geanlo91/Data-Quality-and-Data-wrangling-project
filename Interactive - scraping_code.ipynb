{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Connected to Python 3.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   In Kilometres Cape Town Durban Johannesburg\n",
      "0   Aliwal North       963    722          597\n",
      "1  Beaufort West       459   1147          934\n",
      "2     Beitbridge      1948   1060          547\n",
      "3      Bethlehem      1243    388          259\n",
      "4   Bloemfontein       997    628          394\n",
      "   #               Team  Pl   W  D  L   F   A  GD  Pts  Last 6\n",
      "0  1            Arsenal  18  12  4  2  36  16  20   40     NaN\n",
      "1  2          Liverpool  18  11  6  1  37  16  21   39     NaN\n",
      "2  3        Aston Villa  18  12  3  3  38  22  16   39     NaN\n",
      "3  4  Tottenham Hotspur  18  11  3  4  37  24  13   36     NaN\n",
      "4  5    Manchester City  17  10  4  3  40  20  20   34     NaN\n",
      "   #             Team  Pl   W  D  L   F   A  GD  Pts  Last 6\n",
      "0  1      Real Madrid  18  14  3  1  39  11  28   45     NaN\n",
      "1  2           Girona  18  14  3  1  42  21  21   45     NaN\n",
      "2  3  Atletico Madrid  18  12  2  4  36  19  17   38     NaN\n",
      "3  4        Barcelona  18  11  5  2  34  21  13   38     NaN\n",
      "4  5  Athletic Bilbao  18  10  5  3  34  19  15   35     NaN\n",
      "Saved scraped data to scraped_data.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python311\\site-packages\\tables\\path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'South-Africa-Distance-Chart_content_op_view_id_2938\\nScrapped date: 2023-12-26'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python311\\site-packages\\tables\\path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'premier-league-table\\nScrapped date: 2023-12-26'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python311\\site-packages\\tables\\path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'la-liga-table\\nScrapped date: 2023-12-26'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests \n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import datetime \n",
    "import matplotlib.collections as mcol\n",
    "from matplotlib.legend_handler import HandlerLineCollection, HandlerTuple\n",
    "from matplotlib.lines import Line2D\n",
    "import logging\n",
    "\n",
    "#Logging setup\n",
    "logging.basicConfig(filename='scraping.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')    \n",
    "\n",
    "# Step 1: Read URLs from CSV\n",
    "csv_file = 'web_pages.csv'  # Replace with your CSV file path\n",
    "urls = pd.read_csv(csv_file)\n",
    "\n",
    "# Function to scrape data\n",
    "def scrape_data(urls):\n",
    "    scraped_tables = {}\n",
    "\n",
    "    # Iterate over rows in the CSV file\n",
    "    for _, row in urls.iterrows():  \n",
    "        url = row['URL']\n",
    "\n",
    "        # Get the first column name from CSV file\n",
    "        first_column_name = row['start_column_name']  \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            tables_list = pd.read_html(response.text)\n",
    "            for table in tables_list:\n",
    "\n",
    "                # Check if the first column of the table matches the desired name\n",
    "                if table.columns[0] == first_column_name:\n",
    "                    scraped_tables[url] = table\n",
    "                    logging.info(f\"\\nScraped table from: {url} \\nfirst column: '{first_column_name} \\nscrapped date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "                    \n",
    "                    # Print the first few rows of the table\n",
    "                    print(table.head())  \n",
    "                    break\n",
    "        except ValueError:\n",
    "            logging.error(f\"No tables found in {url}\")\n",
    "            print(f\"No tables found in {url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while scraping {url}: {e}\")\n",
    "    return scraped_tables\n",
    "\n",
    "#Saving scraped data to HDF5 file\n",
    "def save_to_hdf5(scraped_tables, hdf5_file, scrape_date):\n",
    "    with pd.HDFStore(hdf5_file, 'a') as store:\n",
    "        for url, (url, table) in enumerate(scraped_tables.items(), start=1):\n",
    "            if isinstance(table, pd.DataFrame):\n",
    "                group_name = f\"URL_:{url}\" + f\"\\nScrapped date: {scrape_date}\"\n",
    "                store.put(group_name, table)\n",
    "\n",
    "                # Save URL and date as attributes\n",
    "                store.get_storer(group_name).attrs.metadata = {f\"url_{url}\": url, 'date': scrape_date}\n",
    "                logging.info(f\"Saved table from: {url} scrapped date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            else:\n",
    "                logging.error(f\"Skipped saving non-DataFrame data from {url}\")\n",
    "                print(f\"Skipped saving non-DataFrame data from {url}\")\n",
    "           \n",
    "\n",
    "# HDF5 file name\n",
    "hdf5_file = 'scraped_data.h5'\n",
    " \n",
    "#scraping count\n",
    "number_of_times = 1\n",
    "for _ in range(number_of_times):\n",
    "    scrape_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    scraped_data = scrape_data(urls)\n",
    "\n",
    "    # Save to HDF5 and visualize after processing all URLs\n",
    "    save_to_hdf5(scraped_data, hdf5_file, scrape_date)\n",
    "    print(f\"Saved scraped data to {hdf5_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
