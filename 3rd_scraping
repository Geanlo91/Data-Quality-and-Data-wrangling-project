import pandas as pd
import requests
import h5py
import matplotlib.pyplot as plt
import datetime
from datetime import datetime 


# Step 1: Read URLs from CSV
csv_file = 'web_pages.csv'  # Replace with your CSV file path
urls = pd.read_csv(csv_file)

# Function to scrape data
def scrape_data(urls):
    scraped_tables = {}
    for index, row in urls.iterrows():  # Iterate over rows in the CSV file
        url = row['URL']
        first_column_name = row['start_column_name']  # Get the first column name from CSV
        try:
            response = requests.get(url)
            tables_list = pd.read_html(response.text)
            for table in tables_list:
                # Check if the first column of the table matches the desired name
                if table.columns[0] == first_column_name:
                    scraped_tables[url] = table
                    print(f"Scraped table from {url} with first column '{first_column_name}':")
                    print(table.head())  # Print the first few rows of the table
                    break  # Stop after finding the first matching table
        except ValueError:
            print(f"No tables found in {url}")
        except Exception as e:
            print(f"Error occurred while scraping {url}: {e}")
    return scraped_tables

#Clean and convert data to numeric
def clean_and_convert_to_numeric(df):
    for col in df.columns:
        if df[col].dtype == object:
            # Remove the degree symbol and convert to numeric
            df[col] = df[col].str.replace('Â°', '', regex=False).str.strip()
            df[col] = pd.to_numeric(df[col], errors='coerce')
    return df

# Function to save data to HDF5
def save_to_hdf5(scraped_tables, hdf5_file, scrape_date):
    with pd.HDFStore(hdf5_file, 'a') as store:
        for url_counter, (url, table) in enumerate(scraped_tables.items(), start=1):
            # Ensure the table is a DataFrame before attempting to save
            if isinstance(table, pd.DataFrame):
                table_name = f"URL_{url_counter}_date_{scrape_date}"
                store.put(table_name, table)
            else:
                print(f"Skipped saving non-DataFrame data from {url}")

# Function to visualize data
def visualize_data(scraped_tables):
    for url, table in scraped_tables.items():
        if isinstance(table, pd.DataFrame):
            if table.empty or len(table.columns) < 2 or len(table.index) < 4:
                continue  # Skip tables that are empty or have fewer than 2 columns or 4 rows
            
            try:
                # Check if the second column is numeric for plotting
                if pd.api.types.is_numeric_dtype(table.iloc[:, 1]):
                    plt.figure()
                    table.plot(kind='line')  # Line plot for numerical data
                    plt.title(f"Line plot from {url}")
                    plt.show()
                else:
                    # Try a bar plot if the first column is categorical and the second column is numeric
                    if pd.api.types.is_numeric_dtype(table.iloc[:, 1]):
                        plt.figure()
                        table.iloc[:, :2].plot(kind='bar', x=table.columns[0])
                        plt.title(f"Bar plot from {url}")
                        plt.show()
            except Exception as e:
                print(f"Error plotting table from {url}: {e}")
        else:
            print(f"Data from {url} is not a DataFrame and cannot be plotted.")

# HDF5 file name
hdf5_file = 'scraped_data2.h5'

# Example of scraping for multiple days (adjust as needed)
number_of_days = 5  # Replace with your desired number of days
for _ in range(number_of_days):
    scrape_date = datetime.now().strftime("%Y%m%d")
    scraped_data = scrape_data(urls)

    # Clean and convert each scraped DataFrame
    for url, table in scraped_data.items():
        if isinstance(table, pd.DataFrame):
            scraped_data[url] = clean_and_convert_to_numeric(table)

    # Save to HDF5 and visualize after processing all URLs
    save_to_hdf5(scraped_data, hdf5_file, scrape_date)
    visualize_data(scraped_data)

